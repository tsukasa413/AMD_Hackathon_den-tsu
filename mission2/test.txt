usage: lerobot-record [-h] [--config_path str] [--robot str]
                      [--robot.type {so100_follower,bi_so100_follower,hope_jr_hand,hope_jr_arm,koch_follower,so101_follower}]
                      [--robot.left_arm_port str] [--robot.right_arm_port str]
                      [--robot.left_arm_disable_torque_on_disconnect bool]
                      [--robot.left_arm_max_relative_target [float|Dict]]
                      [--robot.left_arm_use_degrees bool]
                      [--robot.right_arm_disable_torque_on_disconnect bool]
                      [--robot.right_arm_max_relative_target [float|Dict]]
                      [--robot.right_arm_use_degrees bool] [--robot.side str]
                      [--robot.id [str]] [--robot.calibration_dir [Path]]
                      [--robot.port str]
                      [--robot.disable_torque_on_disconnect bool]
                      [--robot.max_relative_target [float|Dict]]
                      [--robot.cameras Dict] [--robot.use_degrees bool]
                      [--dataset str] [--dataset.repo_id str]
                      [--dataset.single_task str] [--dataset.root [str|Path]]
                      [--dataset.fps int] [--dataset.episode_time_s int|float]
                      [--dataset.reset_time_s int|float]
                      [--dataset.num_episodes int] [--dataset.video bool]
                      [--dataset.push_to_hub bool] [--dataset.private bool]
                      [--dataset.tags [List]]
                      [--dataset.num_image_writer_processes int]
                      [--dataset.num_image_writer_threads_per_camera int]
                      [--dataset.video_encoding_batch_size int]
                      [--dataset.rename_map Dict] [--teleop str]
                      [--teleop.type {so100_leader,bi_so100_leader,homunculus_glove,homunculus_arm,koch_leader,so101_leader,keyboard,keyboard_ee}]
                      [--teleop.left_arm_port str]
                      [--teleop.right_arm_port str] [--teleop.side str]
                      [--teleop.baud_rate int]
                      [--teleop.gripper_open_pos float] [--teleop.port str]
                      [--teleop.use_degrees bool] [--teleop.id [str]]
                      [--teleop.calibration_dir [Path]]
                      [--teleop.use_gripper bool] [--policy str]
                      [--policy.type {act,diffusion,groot,pi0,pi05,smolvla,tdmpc,vqbet,sac,reward_classifier}]
                      [--policy.replace_final_stride_with_dilation int]
                      [--policy.pre_norm bool] [--policy.dim_model int]
                      [--policy.n_heads int] [--policy.dim_feedforward int]
                      [--policy.feedforward_activation str]
                      [--policy.n_encoder_layers int]
                      [--policy.n_decoder_layers int] [--policy.use_vae bool]
                      [--policy.n_vae_encoder_layers int]
                      [--policy.temporal_ensemble_coeff [float]]
                      [--policy.kl_weight float]
                      [--policy.optimizer_lr_backbone float]
                      [--policy.drop_n_last_frames int]
                      [--policy.use_separate_rgb_encoder_per_camera bool]
                      [--policy.down_dims int [int, ...]]
                      [--policy.kernel_size int] [--policy.n_groups int]
                      [--policy.diffusion_step_embed_dim int]
                      [--policy.use_film_scale_modulation bool]
                      [--policy.noise_scheduler_type str]
                      [--policy.num_train_timesteps int]
                      [--policy.beta_schedule str] [--policy.beta_start float]
                      [--policy.beta_end float] [--policy.prediction_type str]
                      [--policy.clip_sample bool]
                      [--policy.clip_sample_range float]
                      [--policy.do_mask_loss_for_padding bool]
                      [--policy.scheduler_name str]
                      [--policy.image_size int int]
                      [--policy.base_model_path str]
                      [--policy.tokenizer_assets_repo str]
                      [--policy.embodiment_tag str] [--policy.tune_llm bool]
                      [--policy.tune_visual bool]
                      [--policy.tune_projector bool]
                      [--policy.tune_diffusion_model bool]
                      [--policy.lora_rank int] [--policy.lora_alpha int]
                      [--policy.lora_dropout float]
                      [--policy.lora_full_model bool]
                      [--policy.warmup_ratio float] [--policy.use_bf16 bool]
                      [--policy.video_backend str]
                      [--policy.balance_dataset_weights bool]
                      [--policy.balance_trajectory_weights bool]
                      [--policy.dataset_paths [List]]
                      [--policy.output_dir str] [--policy.save_steps int]
                      [--policy.max_steps int] [--policy.batch_size int]
                      [--policy.dataloader_num_workers int]
                      [--policy.report_to str] [--policy.resume bool]
                      [--policy.paligemma_variant str]
                      [--policy.action_expert_variant str]
                      [--policy.dtype str] [--policy.num_inference_steps int]
                      [--policy.time_sampling_beta_alpha float]
                      [--policy.time_sampling_beta_beta float]
                      [--policy.time_sampling_scale float]
                      [--policy.time_sampling_offset float]
                      [--policy.image_resolution int int]
                      [--policy.gradient_checkpointing bool]
                      [--policy.compile_model bool]
                      [--policy.compile_mode str] [--policy.chunk_size int]
                      [--policy.max_state_dim int]
                      [--policy.max_action_dim int]
                      [--policy.resize_imgs_with_padding int int]
                      [--policy.empty_cameras int]
                      [--policy.adapt_to_pi_aloha bool]
                      [--policy.use_delta_joint_actions_aloha bool]
                      [--policy.tokenizer_max_length int]
                      [--policy.num_steps int] [--policy.use_cache bool]
                      [--policy.train_expert_only bool]
                      [--policy.train_state_proj bool]
                      [--policy.optimizer_grad_clip_norm float]
                      [--policy.scheduler_decay_steps int]
                      [--policy.scheduler_decay_lr float]
                      [--policy.vlm_model_name str]
                      [--policy.load_vlm_weights bool]
                      [--policy.add_image_special_tokens bool]
                      [--policy.attention_mode str]
                      [--policy.prefix_length int]
                      [--policy.pad_language_to str]
                      [--policy.num_expert_layers int]
                      [--policy.num_vlm_layers int]
                      [--policy.self_attn_every_n_layers int]
                      [--policy.expert_width_multiplier float]
                      [--policy.min_period float] [--policy.max_period float]
                      [--policy.n_action_repeats int] [--policy.horizon int]
                      [--policy.n_action_steps int]
                      [--policy.q_ensemble_size int] [--policy.mlp_dim int]
                      [--policy.use_mpc bool] [--policy.cem_iterations int]
                      [--policy.max_std float] [--policy.min_std float]
                      [--policy.n_gaussian_samples int]
                      [--policy.n_pi_samples int]
                      [--policy.uncertainty_regularizer_coeff float]
                      [--policy.n_elites int]
                      [--policy.elite_weighting_temperature float]
                      [--policy.gaussian_mean_momentum float]
                      [--policy.max_random_shift_ratio float]
                      [--policy.reward_coeff float]
                      [--policy.expectile_weight float]
                      [--policy.value_coeff float]
                      [--policy.consistency_coeff float]
                      [--policy.advantage_scaling float]
                      [--policy.pi_coeff float]
                      [--policy.temporal_decay_coeff float]
                      [--policy.target_model_momentum float]
                      [--policy.n_action_pred_token int]
                      [--policy.action_chunk_size int]
                      [--policy.vision_backbone str]
                      [--policy.crop_shape [int int]]
                      [--policy.crop_is_random bool]
                      [--policy.pretrained_backbone_weights [str]]
                      [--policy.use_group_norm bool]
                      [--policy.spatial_softmax_num_keypoints int]
                      [--policy.n_vqvae_training_steps int]
                      [--policy.vqvae_n_embed int]
                      [--policy.vqvae_embedding_dim int]
                      [--policy.vqvae_enc_hidden_dim int]
                      [--policy.gpt_block_size int]
                      [--policy.gpt_input_dim int]
                      [--policy.gpt_output_dim int] [--policy.gpt_n_layer int]
                      [--policy.gpt_n_head int] [--policy.gpt_hidden_dim int]
                      [--policy.dropout float]
                      [--policy.offset_loss_weight float]
                      [--policy.primary_code_loss_weight float]
                      [--policy.secondary_code_loss_weight float]
                      [--policy.bet_softmax_temperature float]
                      [--policy.sequentially_select bool]
                      [--policy.optimizer_lr float]
                      [--policy.optimizer_betas Any]
                      [--policy.optimizer_eps float]
                      [--policy.optimizer_weight_decay float]
                      [--policy.optimizer_vqvae_lr float]
                      [--policy.optimizer_vqvae_weight_decay float]
                      [--policy.scheduler_warmup_steps int]
                      [--policy.dataset_stats [Dict]]
                      [--policy.storage_device str]
                      [--policy.vision_encoder_name [str]]
                      [--policy.freeze_vision_encoder bool]
                      [--policy.image_encoder_hidden_dim int]
                      [--policy.shared_encoder bool]
                      [--policy.num_discrete_actions [int]]
                      [--policy.online_steps int]
                      [--policy.online_buffer_capacity int]
                      [--policy.offline_buffer_capacity int]
                      [--policy.async_prefetch bool]
                      [--policy.online_step_before_learning int]
                      [--policy.policy_update_freq int]
                      [--policy.discount float]
                      [--policy.temperature_init float]
                      [--policy.num_critics int]
                      [--policy.num_subsample_critics [int]]
                      [--policy.critic_lr float] [--policy.actor_lr float]
                      [--policy.temperature_lr float]
                      [--policy.critic_target_update_weight float]
                      [--policy.utd_ratio int]
                      [--policy.state_encoder_hidden_dim int]
                      [--policy.target_entropy [float]]
                      [--policy.use_backup_entropy bool]
                      [--critic_network_kwargs str]
                      [--policy.critic_network_kwargs.hidden_dims List]
                      [--policy.critic_network_kwargs.activate_final bool]
                      [--policy.critic_network_kwargs.final_activation [str]]
                      [--actor_network_kwargs str]
                      [--policy.actor_network_kwargs.hidden_dims List]
                      [--policy.actor_network_kwargs.activate_final bool]
                      [--policy_kwargs str]
                      [--policy.policy_kwargs.use_tanh_squash bool]
                      [--policy.policy_kwargs.std_min float]
                      [--policy.policy_kwargs.std_max float]
                      [--policy.policy_kwargs.init_final float]
                      [--discrete_critic_network_kwargs str]
                      [--policy.discrete_critic_network_kwargs.hidden_dims List]
                      [--policy.discrete_critic_network_kwargs.activate_final bool]
                      [--policy.discrete_critic_network_kwargs.final_activation [str]]
                      [--actor_learner_config str]
                      [--policy.actor_learner_config.learner_host str]
                      [--policy.actor_learner_config.learner_port int]
                      [--policy.actor_learner_config.policy_parameters_push_frequency int]
                      [--policy.actor_learner_config.queue_get_timeout float]
                      [--concurrency str] [--policy.concurrency.actor str]
                      [--policy.concurrency.learner str]
                      [--policy.use_torch_compile bool]
                      [--policy.n_obs_steps int]
                      [--policy.input_features Dict]
                      [--policy.output_features Dict] [--policy.device str]
                      [--policy.use_amp bool] [--policy.push_to_hub bool]
                      [--policy.repo_id [str]] [--policy.private [bool]]
                      [--policy.tags [List]] [--policy.license [str]]
                      [--policy.pretrained_path [Path]] [--policy.name str]
                      [--policy.num_classes int] [--policy.hidden_dim int]
                      [--policy.latent_dim int]
                      [--policy.image_embedding_pooling_dim int]
                      [--policy.dropout_rate float] [--policy.model_name str]
                      [--policy.model_type str] [--policy.num_cameras int]
                      [--policy.learning_rate float]
                      [--policy.weight_decay float]
                      [--policy.grad_clip_norm float]
                      [--policy.normalization_mapping Dict]
                      [--display_data bool] [--play_sounds bool]
                      [--resume bool]

options:
  -h, --help            show this help message and exit
  --config_path str     Path for a config file to parse with draccus (default:
                        None)
  --robot str           Config file for robot (default: None)
  --dataset str         Config file for dataset (default: None)
  --teleop str          Config file for teleop (default: None)
  --policy str          Config file for policy (default: None)
  --critic_network_kwargs str
                        Config file for critic_network_kwargs (default: None)
  --actor_network_kwargs str
                        Config file for actor_network_kwargs (default: None)
  --policy_kwargs str   Config file for policy_kwargs (default: None)
  --discrete_critic_network_kwargs str
                        Config file for discrete_critic_network_kwargs
                        (default: None)
  --actor_learner_config str
                        Config file for actor_learner_config (default: None)
  --concurrency str     Config file for concurrency (default: None)

RecordConfig:

  --display_data bool   Display all cameras on screen (default: False)
  --play_sounds bool    Use vocal synthesis to read events. (default: True)
  --resume bool         Resume recording on an existing dataset. (default:
                        False)

RobotConfig ['robot']:

  --robot.type {so100_follower,bi_so100_follower,hope_jr_hand,hope_jr_arm,koch_follower,so101_follower}
                        Which type of RobotConfig ['robot'] to use (default:
                        None)

SO100FollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [float|Dict]
                        `max_relative_target` limits the magnitude of the
                        relative positional target vector for safety purposes.
                        Set this to a positive scalar to have the same value
                        for all motors, or a dictionary that maps motor names
                        to the max_relative_target value for that motor.
                        (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous
                        policies/dataset (default: False)

BiSO100FollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.left_arm_port str
  --robot.right_arm_port str
  --robot.left_arm_disable_torque_on_disconnect bool
                        Optional (default: True)
  --robot.left_arm_max_relative_target [float|Dict]
  --robot.left_arm_use_degrees bool
  --robot.right_arm_disable_torque_on_disconnect bool
  --robot.right_arm_max_relative_target [float|Dict]
  --robot.right_arm_use_degrees bool
  --robot.cameras Dict  cameras (shared between both arms) (default: {})

HopeJrHandConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the hand (default: None)
  --robot.side str      "left" / "right" (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.cameras Dict  

HopeJrArmConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the hand (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [float|Dict]
                        `max_relative_target` limits the magnitude of the
                        relative positional target vector for safety purposes.
                        Set this to a positive scalar to have the same value
                        for all motors, or a dictionary that maps motor names
                        to the max_relative_target value for that motor.
                        (default: None)
  --robot.cameras Dict  

KochFollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [float|Dict]
                        `max_relative_target` limits the magnitude of the
                        relative positional target vector for safety purposes.
                        Set this to a positive scalar to have the same value
                        for all motors, or a dictionary that maps motor names
                        to the max_relative_target value for that motor.
                        (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous
                        policies/dataset (default: False)

SO101FollowerConfig ['robot']:

  --robot.id [str]      Allows to distinguish between different robots of the
                        same type (default: None)
  --robot.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --robot.port str      Port to connect to the arm (default: None)
  --robot.disable_torque_on_disconnect bool
  --robot.max_relative_target [float|Dict]
                        `max_relative_target` limits the magnitude of the
                        relative positional target vector for safety purposes.
                        Set this to a positive scalar to have the same value
                        for all motors, or a dictionary that maps motor names
                        to the max_relative_target value for that motor.
                        (default: None)
  --robot.cameras Dict  cameras (default: {})
  --robot.use_degrees bool
                        Set to `True` for backward compatibility with previous
                        policies/dataset (default: False)

DatasetRecordConfig ['dataset']:

  --dataset.repo_id str
                        Dataset identifier. By convention it should match
                        '{hf_username}/{dataset_name}' (e.g. `lerobot/test`).
                        (default: None)
  --dataset.single_task str
                        A short but accurate description of the task performed
                        during the recording (e.g. "Pick the Lego block and
                        drop it in the box on the right.") (default: None)
  --dataset.root [str|Path]
                        Root directory where the dataset will be stored (e.g.
                        'dataset/path'). (default: None)
  --dataset.fps int     Limit the frames per second. (default: 30)
  --dataset.episode_time_s int|float
                        Number of seconds for data recording for each episode.
                        (default: 60)
  --dataset.reset_time_s int|float
                        Number of seconds for resetting the environment after
                        each episode. (default: 60)
  --dataset.num_episodes int
                        Number of episodes to record. (default: 50)
  --dataset.video bool  Encode frames in the dataset into video (default:
                        True)
  --dataset.push_to_hub bool
                        Upload dataset to Hugging Face hub. (default: True)
  --dataset.private bool
                        Upload on private repository on the Hugging Face hub.
                        (default: False)
  --dataset.tags [List]
                        Add tags to your dataset on the hub. (default: None)
  --dataset.num_image_writer_processes int
                        Number of subprocesses handling the saving of frames
                        as PNG. Set to 0 to use threads only; set to â‰¥1 to use
                        subprocesses, each using threads to write images. The
                        best number of processes and threads depends on your
                        system. We recommend 4 threads per camera with 0
                        processes. If fps is unstable, adjust the thread
                        count. If still unstable, try using 1 or more
                        subprocesses. (default: 0)
  --dataset.num_image_writer_threads_per_camera int
                        Number of threads writing the frames as png images on
                        disk, per camera. Too many threads might cause
                        unstable teleoperation fps due to main thread being
                        blocked. Not enough threads might cause low camera
                        fps. (default: 4)
  --dataset.video_encoding_batch_size int
                        Number of episodes to record before batch encoding
                        videos Set to 1 for immediate encoding (default
                        behavior), or higher for batched encoding (default: 1)
  --dataset.rename_map Dict
                        Rename map for the observation to override the image
                        and state keys (default: {})

Optional ['teleop']:
  Whether to control the robot with a teleoperator

TeleoperatorConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.type {so100_leader,bi_so100_leader,homunculus_glove,homunculus_arm,koch_leader,so101_leader,keyboard,keyboard_ee}
                        Which type of TeleoperatorConfig ['teleop'] to use
                        (default: None)

SO100LeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)

BiSO100LeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.left_arm_port str
  --teleop.right_arm_port str

HomunculusGloveConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the glove (default: None)
  --teleop.side str     "left" / "right" (default: None)
  --teleop.baud_rate int

HomunculusArmConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.baud_rate int

KochLeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.gripper_open_pos float
                        Sets the arm in torque mode with the gripper motor set
                        to this value. This makes it possible to squeeze the
                        gripper and have it spring back to an open position on
                        its own. (default: 50.0)

SO101LeaderConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.port str     Port to connect to the arm (default: None)
  --teleop.use_degrees bool

KeyboardTeleopConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)

KeyboardEndEffectorTeleopConfig ['teleop']:
  Whether to control the robot with a teleoperator

  --teleop.id [str]     Allows to distinguish between different teleoperators
                        of the same type (default: None)
  --teleop.calibration_dir [Path]
                        Directory to store calibration file (default: None)
  --teleop.use_gripper bool

Optional ['policy']:
  Whether to control the robot with a policy

PreTrainedConfig ['policy']:
  Whether to control the robot with a policy

  --policy.type {act,diffusion,groot,pi0,pi05,smolvla,tdmpc,vqbet,sac,reward_classifier}
                        Which type of PreTrainedConfig ['policy'] to use
                        (default: None)

ACTConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.normalization_mapping Dict
  --policy.vision_backbone str
  --policy.pretrained_backbone_weights [str]
  --policy.replace_final_stride_with_dilation int
  --policy.pre_norm bool
  --policy.dim_model int
  --policy.n_heads int  
  --policy.dim_feedforward int
  --policy.feedforward_activation str
  --policy.n_encoder_layers int
  --policy.n_decoder_layers int
  --policy.use_vae bool
  --policy.latent_dim int
  --policy.n_vae_encoder_layers int
  --policy.temporal_ensemble_coeff [float]
  --policy.dropout float
  --policy.kl_weight float
  --policy.optimizer_lr float
                        Training preset (default: 1e-05)
  --policy.optimizer_weight_decay float
  --policy.optimizer_lr_backbone float

DiffusionConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.normalization_mapping Dict
  --policy.drop_n_last_frames int
                        horizon - n_action_steps - n_obs_steps + 1 (default:
                        7)
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.use_separate_rgb_encoder_per_camera bool
  --policy.down_dims int [int, ...]
  --policy.kernel_size int
  --policy.n_groups int
  --policy.diffusion_step_embed_dim int
  --policy.use_film_scale_modulation bool
  --policy.noise_scheduler_type str
                        Noise scheduler. (default: DDPM)
  --policy.num_train_timesteps int
  --policy.beta_schedule str
  --policy.beta_start float
  --policy.beta_end float
  --policy.prediction_type str
  --policy.clip_sample bool
  --policy.clip_sample_range float
  --policy.num_inference_steps [int]
  --policy.do_mask_loss_for_padding bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.scheduler_name str
  --policy.scheduler_warmup_steps int

GrootConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Basic policy settings (default: 1)
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.max_state_dim int
                        Dimension settings (must match pretrained GR00T model
                        expectations) Maximum state dimension. Shorter states
                        will be zero-padded. (default: 64)
  --policy.max_action_dim int
                        Maximum action dimension. Shorter actions will be
                        zero-padded. (default: 32)
  --policy.normalization_mapping Dict
                        Normalization (start with identity, adjust as needed)
                        (default: {'VISUAL': <NormalizationMode.IDENTITY:
                        'IDENTITY'>, 'STATE': <NormalizationMode.MEAN_STD:
                        'MEAN_STD'>, 'ACTION': <NormalizationMode.MEAN_STD:
                        'MEAN_STD'>})
  --policy.image_size int int
                        Image preprocessing (adjust to match Groot's expected
                        input) (default: (224, 224))
  --policy.base_model_path str
                        Groot-specific model parameters (from
                        groot_finetune_script.py) Path or HuggingFace model ID
                        for the base Groot model (default:
                        nvidia/GR00T-N1.5-3B)
  --policy.tokenizer_assets_repo str
                        HF repo ID (or local path) that hosts vocab.json and
                        merges.txt for Eagle tokenizer. (default:
                        lerobot/eagle2hg-processor-groot-n1p5)
  --policy.embodiment_tag str
                        Embodiment tag to use for training (e.g.
                        'new_embodiment', 'gr1') (default: new_embodiment)
  --policy.tune_llm bool
                        Fine-tuning control arguments Whether to fine-tune the
                        llm backbone (default: False)
  --policy.tune_visual bool
                        Whether to fine-tune the vision tower (default: False)
  --policy.tune_projector bool
                        Whether to fine-tune the projector (default: True)
  --policy.tune_diffusion_model bool
                        Whether to fine-tune the diffusion model (default:
                        True)
  --policy.lora_rank int
                        LoRA parameters (from groot_finetune_script.py) Rank
                        for the LORA model. If 0, no LORA will be used.
                        (default: 0)
  --policy.lora_alpha int
                        Alpha value for the LORA model (default: 16)
  --policy.lora_dropout float
                        Dropout rate for the LORA model (default: 0.1)
  --policy.lora_full_model bool
                        Whether to use the full model for LORA (default:
                        False)
  --policy.optimizer_lr float
                        Training parameters (matching
                        groot_finetune_script.py) (default: 0.0001)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.warmup_ratio float
  --policy.use_bf16 bool
  --policy.video_backend str
                        Dataset parameters Video backend to use for training
                        ('decord' or 'torchvision_av') (default: decord)
  --policy.balance_dataset_weights bool
                        Whether to balance dataset weights in mixture datasets
                        (default: True)
  --policy.balance_trajectory_weights bool
                        Whether to sample trajectories weighted by their
                        length (default: True)
  --policy.dataset_paths [List]
                        Optional dataset paths for delegating training to
                        Isaac-GR00T runner (default: None)
  --policy.output_dir str
  --policy.save_steps int
  --policy.max_steps int
  --policy.batch_size int
  --policy.dataloader_num_workers int
  --policy.report_to str
  --policy.resume bool  

PI0Config ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        Device to use for the model (None = auto-detect)
                        (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.paligemma_variant str
  --policy.action_expert_variant str
  --policy.dtype str    Options: "bfloat16", "float32" (default: float32)
  --policy.chunk_size int
                        Number of action steps to predict, in openpi called
                        "action_horizon" (default: 50)
  --policy.n_action_steps int
                        Number of action steps to execute (default: 50)
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded to
                        these dimensions (default: 32)
  --policy.max_action_dim int
  --policy.num_inference_steps int
                        Number of denoising steps during inference (default:
                        10)
  --policy.time_sampling_beta_alpha float
  --policy.time_sampling_beta_beta float
  --policy.time_sampling_scale float
  --policy.time_sampling_offset float
  --policy.min_period float
  --policy.max_period float
  --policy.image_resolution int int
                        see openpi `preprocessing_pytorch.py` (default: (224,
                        224))
  --policy.empty_cameras int
                        Add empty images. Used to add empty cameras when no
                        image features are present. (default: 0)
  --policy.normalization_mapping Dict
                        Normalization (default: {'VISUAL':
                        <NormalizationMode.IDENTITY: 'IDENTITY'>, 'STATE':
                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>, 'ACTION':
                        <NormalizationMode.MEAN_STD: 'MEAN_STD'>})
  --policy.gradient_checkpointing bool
                        Enable gradient checkpointing for memory optimization
                        (default: False)
  --policy.compile_model bool
                        Whether to use torch.compile for model optimization
                        (default: False)
  --policy.compile_mode str
                        Torch compile mode (default: max-autotune)
  --policy.optimizer_lr float
                        see openpi `CosineDecaySchedule: peak_lr` (default:
                        2.5e-05)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_grad_clip_norm float
  --policy.scheduler_warmup_steps int
                        Scheduler settings: see openpi `CosineDecaySchedule`
                        Note: These will auto-scale if --steps <
                        scheduler_decay_steps For example, --steps=3000 will
                        scale warmup to 100 and decay to 3000 (default: 1000)
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float
  --policy.tokenizer_max_length int
                        see openpi `__post_init__` (default: 48)

PI05Config ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        Device to use for the model (None = auto-detect)
                        (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.paligemma_variant str
  --policy.action_expert_variant str
  --policy.dtype str    Options: "bfloat16", "float32" (default: float32)
  --policy.chunk_size int
                        Number of action steps to predict, in openpi called
                        "action_horizon" (default: 50)
  --policy.n_action_steps int
                        Number of action steps to execute (default: 50)
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded to
                        these dimensions (default: 32)
  --policy.max_action_dim int
  --policy.num_inference_steps int
                        Flow matching parameters: see openpi `PI0Pytorch`
                        (default: 10)
  --policy.time_sampling_beta_alpha float
  --policy.time_sampling_beta_beta float
  --policy.time_sampling_scale float
  --policy.time_sampling_offset float
  --policy.min_period float
  --policy.max_period float
  --policy.image_resolution int int
                        see openpi `preprocessing_pytorch.py` (default: (224,
                        224))
  --policy.empty_cameras int
                        Add empty images. Used to add empty cameras when no
                        image features are present. (default: 0)
  --policy.tokenizer_max_length int
                        see openpi `__post_init__` (default: 200)
  --policy.normalization_mapping Dict
  --policy.gradient_checkpointing bool
                        Enable gradient checkpointing for memory optimization
                        (default: False)
  --policy.compile_model bool
                        Whether to use torch.compile for model optimization
                        (default: False)
  --policy.compile_mode str
                        Torch compile mode (default: max-autotune)
  --policy.optimizer_lr float
                        see openpi `CosineDecaySchedule: peak_lr` (default:
                        2.5e-05)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_grad_clip_norm float
  --policy.scheduler_warmup_steps int
                        Scheduler settings: see openpi `CosineDecaySchedule`
                        Note: These will auto-scale if --steps <
                        scheduler_decay_steps For example, --steps=3000 will
                        scale warmup to 100 and decay to 3000 (default: 1000)
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float

SmolVLAConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.chunk_size int
  --policy.n_action_steps int
  --policy.normalization_mapping Dict
  --policy.max_state_dim int
                        Shorter state and action vectors will be padded
                        (default: 32)
  --policy.max_action_dim int
  --policy.resize_imgs_with_padding int int
                        Image preprocessing (default: (512, 512))
  --policy.empty_cameras int
                        Add empty images. Used by smolvla_aloha_sim which adds
                        the empty left and right wrist cameras in addition to
                        the top camera. (default: 0)
  --policy.adapt_to_pi_aloha bool
                        Converts the joint and gripper values from the
                        standard Aloha space to the space used by the pi
                        internal runtime which was used to train the base
                        model. (default: False)
  --policy.use_delta_joint_actions_aloha bool
                        Converts joint dimensions to deltas with respect to
                        the current state before passing to the model. Gripper
                        dimensions will remain in absolute values. (default:
                        False)
  --policy.tokenizer_max_length int
                        Tokenizer (default: 48)
  --policy.num_steps int
                        Decoding (default: 10)
  --policy.use_cache bool
                        Attention utils (default: True)
  --policy.freeze_vision_encoder bool
                        Finetuning settings (default: True)
  --policy.train_expert_only bool
  --policy.train_state_proj bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas float float
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_grad_clip_norm float
  --policy.scheduler_warmup_steps int
  --policy.scheduler_decay_steps int
  --policy.scheduler_decay_lr float
  --policy.vlm_model_name str
                        Select the VLM backbone. (default:
                        HuggingFaceTB/SmolVLM2-500M-Video-Instruct)
  --policy.load_vlm_weights bool
                        Set to True in case of training the expert from
                        scratch. True when init from pretrained SmolVLA
                        weights (default: False)
  --policy.add_image_special_tokens bool
                        Whether to use special image tokens around image
                        features. (default: False)
  --policy.attention_mode str
  --policy.prefix_length int
  --policy.pad_language_to str
                        "max_length" (default: longest)
  --policy.num_expert_layers int
                        Less or equal to 0 is the default where the action
                        expert has the same number of layers of VLM. Otherwise
                        the expert have less layers. (default: -1)
  --policy.num_vlm_layers int
                        Number of layers used in the VLM (first num_vlm_layers
                        layers) (default: 16)
  --policy.self_attn_every_n_layers int
                        Interleave SA layers each self_attn_every_n_layers
                        (default: 2)
  --policy.expert_width_multiplier float
                        The action expert hidden size (wrt to the VLM)
                        (default: 0.75)
  --policy.min_period float
                        sensitivity range for the timestep used in sine-cosine
                        positional encoding (default: 0.004)
  --policy.max_period float

TDMPCConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
                        Input / output structure. (default: 1)
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.n_action_repeats int
  --policy.horizon int  
  --policy.n_action_steps int
  --policy.normalization_mapping Dict
  --policy.image_encoder_hidden_dim int
  --policy.state_encoder_hidden_dim int
  --policy.latent_dim int
  --policy.q_ensemble_size int
  --policy.mlp_dim int  
  --policy.discount float
  --policy.use_mpc bool
  --policy.cem_iterations int
  --policy.max_std float
  --policy.min_std float
  --policy.n_gaussian_samples int
  --policy.n_pi_samples int
  --policy.uncertainty_regularizer_coeff float
  --policy.n_elites int
  --policy.elite_weighting_temperature float
  --policy.gaussian_mean_momentum float
  --policy.max_random_shift_ratio float
  --policy.reward_coeff float
  --policy.expectile_weight float
  --policy.value_coeff float
  --policy.consistency_coeff float
  --policy.advantage_scaling float
  --policy.pi_coeff float
  --policy.temporal_decay_coeff float
  --policy.target_model_momentum float
  --policy.optimizer_lr float
                        Training presets (default: 0.0003)

VQBeTConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device [str]
                        e.g. "cuda", "cuda:0", "cpu", or "mps" (default: None)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.n_action_pred_token int
  --policy.action_chunk_size int
  --policy.normalization_mapping Dict
  --policy.vision_backbone str
  --policy.crop_shape [int int]
  --policy.crop_is_random bool
  --policy.pretrained_backbone_weights [str]
  --policy.use_group_norm bool
  --policy.spatial_softmax_num_keypoints int
  --policy.n_vqvae_training_steps int
  --policy.vqvae_n_embed int
  --policy.vqvae_embedding_dim int
  --policy.vqvae_enc_hidden_dim int
  --policy.gpt_block_size int
  --policy.gpt_input_dim int
  --policy.gpt_output_dim int
  --policy.gpt_n_layer int
  --policy.gpt_n_head int
  --policy.gpt_hidden_dim int
  --policy.dropout float
  --policy.offset_loss_weight float
  --policy.primary_code_loss_weight float
  --policy.secondary_code_loss_weight float
  --policy.bet_softmax_temperature float
  --policy.sequentially_select bool
  --policy.optimizer_lr float
                        Training presets (default: 0.0001)
  --policy.optimizer_betas Any
  --policy.optimizer_eps float
  --policy.optimizer_weight_decay float
  --policy.optimizer_vqvae_lr float
  --policy.optimizer_vqvae_weight_decay float
  --policy.scheduler_warmup_steps int

SACConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device str   Architecture specifics Device to run the model on
                        (e.g., "cuda", "cpu") (default: cpu)
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.normalization_mapping Dict
                        Mapping of feature types to normalization modes
                        (default: {'VISUAL': <NormalizationMode.MEAN_STD:
                        'MEAN_STD'>, 'STATE': <NormalizationMode.MIN_MAX:
                        'MIN_MAX'>, 'ENV': <NormalizationMode.MIN_MAX:
                        'MIN_MAX'>, 'ACTION': <NormalizationMode.MIN_MAX:
                        'MIN_MAX'>})
  --policy.dataset_stats [Dict]
                        Statistics for normalizing different types of inputs
                        (default: {'observation.image': {'mean': [0.485,
                        0.456, 0.406], 'std': [0.229, 0.224, 0.225]},
                        'observation.state': {'min': [0.0, 0.0], 'max': [1.0,
                        1.0]}, 'action': {'min': [0.0, 0.0, 0.0], 'max': [1.0,
                        1.0, 1.0]}})
  --policy.storage_device str
                        Device to store the model on (default: cpu)
  --policy.vision_encoder_name [str]
                        Name of the vision encoder model (Set to
                        "helper2424/resnet10" for hil serl resnet10) (default:
                        None)
  --policy.freeze_vision_encoder bool
                        Whether to freeze the vision encoder during training
                        (default: True)
  --policy.image_encoder_hidden_dim int
                        Hidden dimension size for the image encoder (default:
                        32)
  --policy.shared_encoder bool
                        Whether to use a shared encoder for actor and critic
                        (default: True)
  --policy.num_discrete_actions [int]
                        Number of discrete actions, eg for gripper actions
                        (default: None)
  --policy.image_embedding_pooling_dim int
                        Dimension of the image embedding pooling (default: 8)
  --policy.online_steps int
                        Training parameter Number of steps for online training
                        (default: 1000000)
  --policy.online_buffer_capacity int
                        Capacity of the online replay buffer (default: 100000)
  --policy.offline_buffer_capacity int
                        Capacity of the offline replay buffer (default:
                        100000)
  --policy.async_prefetch bool
                        Whether to use asynchronous prefetching for the
                        buffers (default: False)
  --policy.online_step_before_learning int
                        Number of steps before learning starts (default: 100)
  --policy.policy_update_freq int
                        Frequency of policy updates (default: 1)
  --policy.discount float
                        SAC algorithm parameters Discount factor for the SAC
                        algorithm (default: 0.99)
  --policy.temperature_init float
                        Initial temperature value (default: 1.0)
  --policy.num_critics int
                        Number of critics in the ensemble (default: 2)
  --policy.num_subsample_critics [int]
                        Number of subsampled critics for training (default:
                        None)
  --policy.critic_lr float
                        Learning rate for the critic network (default: 0.0003)
  --policy.actor_lr float
                        Learning rate for the actor network (default: 0.0003)
  --policy.temperature_lr float
                        Learning rate for the temperature parameter (default:
                        0.0003)
  --policy.critic_target_update_weight float
                        Weight for the critic target update (default: 0.005)
  --policy.utd_ratio int
                        Update-to-data ratio for the UTD algorithm (If you
                        want enable utd_ratio, you need to set it to >1)
                        (default: 1)
  --policy.state_encoder_hidden_dim int
                        Hidden dimension size for the state encoder (default:
                        256)
  --policy.latent_dim int
                        Dimension of the latent space (default: 256)
  --policy.target_entropy [float]
                        Target entropy for the SAC algorithm (default: None)
  --policy.use_backup_entropy bool
                        Whether to use backup entropy for the SAC algorithm
                        (default: True)
  --policy.grad_clip_norm float
                        Gradient clipping norm for the SAC algorithm (default:
                        40.0)
  --policy.use_torch_compile bool
                        Optimizations (default: True)

CriticNetworkConfig ['policy.critic_network_kwargs']:
  Network configuration
  Configuration for the critic network architecture

  --policy.critic_network_kwargs.hidden_dims List
  --policy.critic_network_kwargs.activate_final bool
  --policy.critic_network_kwargs.final_activation [str]

ActorNetworkConfig ['policy.actor_network_kwargs']:
  Configuration for the actor network architecture

  --policy.actor_network_kwargs.hidden_dims List
  --policy.actor_network_kwargs.activate_final bool

PolicyConfig ['policy.policy_kwargs']:
  Configuration for the policy parameters

  --policy.policy_kwargs.use_tanh_squash bool
  --policy.policy_kwargs.std_min float
  --policy.policy_kwargs.std_max float
  --policy.policy_kwargs.init_final float

CriticNetworkConfig ['policy.discrete_critic_network_kwargs']:
  Configuration for the discrete critic network

  --policy.discrete_critic_network_kwargs.hidden_dims List
  --policy.discrete_critic_network_kwargs.activate_final bool
  --policy.discrete_critic_network_kwargs.final_activation [str]

ActorLearnerConfig ['policy.actor_learner_config']:
  Configuration for actor-learner architecture

  --policy.actor_learner_config.learner_host str
  --policy.actor_learner_config.learner_port int
  --policy.actor_learner_config.policy_parameters_push_frequency int
  --policy.actor_learner_config.queue_get_timeout float

ConcurrencyConfig ['policy.concurrency']:
  Configuration for concurrency settings (you can use threads or processes for the actor and learner)

  --policy.concurrency.actor str
  --policy.concurrency.learner str

RewardClassifierConfig ['policy']:
  Whether to control the robot with a policy

  --policy.n_obs_steps int
  --policy.input_features Dict
  --policy.output_features Dict
  --policy.device str   
  --policy.use_amp bool
                        `use_amp` determines whether to use Automatic Mixed
                        Precision (AMP) for training and evaluation. With AMP,
                        automatic gradient scaling is used. (default: False)
  --policy.push_to_hub bool
                        type: ignore[assignment] # TODO: use a different name
                        to avoid override (default: True)
  --policy.repo_id [str]
  --policy.private [bool]
                        Upload on private repository on the Hugging Face hub.
                        (default: None)
  --policy.tags [List]  Add tags to your policy on the hub. (default: None)
  --policy.license [str]
                        Add tags to your policy on the hub. (default: None)
  --policy.pretrained_path [Path]
                        Either the repo ID of a model hosted on the Hub or a
                        path to a directory containing weights saved using
                        `Policy.save_pretrained`. If not provided, the policy
                        is initialized from scratch. (default: None)
  --policy.name str     
  --policy.num_classes int
  --policy.hidden_dim int
  --policy.latent_dim int
  --policy.image_embedding_pooling_dim int
  --policy.dropout_rate float
  --policy.model_name str
  --policy.model_type str
                        "transformer" or "cnn" (default: cnn)
  --policy.num_cameras int
  --policy.learning_rate float
  --policy.weight_decay float
  --policy.grad_clip_norm float
  --policy.normalization_mapping Dict
